# R1: What is computer ethics?

**A proposed definition**

Computer ethics is the analysis of the **nature** and **social impact** for **computer technology** (software, hardware, networks connecting computers and computers). A goal of computer ethics is to **determine policies** to guide our actions regarding how computer technology should be used, since there are no policies or existing ones seem inadequate. It includes considerations for both personal and social policies. A major difficulty is that along with a **policy vacuum** there is often a **conceptual vacuum**. An analysis is needed in order to provide a coherent conceptual framework within which formulate a policy for action. Much of the important work of computer ethics consist in proposing conceptual framework for understanding ethical problem. The **conceptualization** will not only affect how a policy will be applied but also what the facts (to which it is applied) are. The formulation of a policy lead to the **reconsideration** of **old values**, since new possibility are provided by the computer technology. It forces us to discover and make explicit what our value preferences are. Not all ethical situations involving computers are central to computer ethics, this means that computer ethics has a special status. It is a **dynamic** and **complex** field of study that does not have a fixed set of rules. It requires us to think about the nature of computer technology and our values. It is a field between science and ethics, and depends on them, but it is also a discipline.

**The revolutionary machine**

It is often said that computer revolution is taking place, but what makes computer revolutionary? The word “revolutionary” has been devalued, even minor technological improvements are called revolutionary. One feature that makes computer revolutionary is that they are **affordable** and **abundant**, just think about products that don’t look like computers (watches, automobiles, ..). Another feature is the **engineering advances** such as increases in speed and memory along with decreases in size. Both these features are not sufficient to justify a technological revolution and only serve as **enabling** **conditions** for the **spread** of computer revolution. The essence of computer revolution is found in the nature of the computer itself. Computers are **logically malleable** since they can be molded to do any activity (by changes in hardware and software) that can be characterized in terms of **inputs**, **outputs** and **connecting logical operations** (steps that take a computer from one state to another). The potential applications of computer technology appear **limitless**, its limit could be consider our **creativity**. Understanding this logical malleability is essential to understanding the power of the technological revolution and is also important in setting policies for the use of computers. Logical malleability has both a **syntactic** and a **semantic** dimension: syntactically the logic of computer is malleable in terms of the number and variety of possibile states and operations, semantically in the states of the computers that can represent anything. Computer manipulate symbols but don’t care what the symbol represent, so there is **no ontological basis** for giving preference to numerical applications over nonnumerical applications. Often the mistake is to consider the mathematical notation as the essence of a computer and then use this conception to make judgement about the appropriate use of computers. Our conceptions of computers technology will affect our policies for using it.

**Anatomy of the computer revolution** Since the **computer revolution** is still in progress, it is difficult to get a perspective on its development. The idea is that computer revolution will take place in two stage:

* **Introduction** stage: that has been occurring during the last forty years during which computers have been created and refined. Computers are understood as tools for doing standard jobs.
* **Permeation** stage (**computerization**): in which computer technology will become an integral part of institutions throughout our society. In the coming decades many human activities and social institutions will be transformed.

In the future various aspects of our **daily work** will be **transformed**. Traditional work may no longer be defined as something that normally happens at a specific time or a specific place. It may become less doing a job than instructing a computer to do a job. As the concept of work begins to change, the **values** associated with the old concept will have to be **reexamined**. Others area that will be affected by computers are educations or finance. Both their traditional values will be challenged and new ones will arise. The point is that these transformation will leave us with policy and conceptual vacuums about how to use computer technology. Such policy and conceptual vacuums are the marks of basic problems within computer ethics. Therefore, computer ethics is a field of substantial practical importance.

**The invisibility factor**

Most of the time and under most conditions **computer operations** are **invisible** (internal processing). This often generates policy vacuums about how to use computer technology. Three kinds of invisibility can be distinguished:

* **Invisible abuse**: the **intentional use** of the **invisible operations** of a computer to engage in unethical conduct (steal excess interest from the bank, invasion of property and privacy of others, use of computer for surveillance in a workplace). The question is what policy to institute in order to best detect and prevent such abuse.
* **Presence** of **invisible programming values**, which are **embedded** in a computer program: a request for a computer program is made at a level of abstraction usually far from the details of the actual programming language, thus the programmer is forced to make some value judgment about what is important and what it is not. Sometimes invisible programming values are so invisible that even the programmers are unaware of them. Programs may be based on **implicit assumptions** which don’t become obvious until there is a crisis.
* **Invisible complex calculations**: computers today are capable of enormous calculations that are too complex for human inspection and understanding. Even if a program is understood, its calculations my be incomprehensible. The issue is how much we should trust a computer’s invisible calculations.

A partial solution to the invisibility problem may lie with the computers themselves since they have the ability to locate and display hidden information. Computer can make the invisible visible. In terms of efficiency the invisibility factor is a blessing, but is this invisibility that makes us vulnerable. The challenge for computer ethics is to formulate policies which will help us deal with this dilemma and decide when to or not to trust computer.

***

# R2: The Responsibilities of Engineers.

Responsibility means being held accountable for your actions and for the effects of your actions. The making of choices, the taking of decisions but also failing to act are all things that we regard as types of actions. Different kinds of responsibility can be distinguished:

* **Active** responsibility: before something has happened
* **Passive** responsibility: applicable after something (undesirable) has happened.
* **Role** responsibility: the responsibility that is based on the role one has or plays in a certain situation. Roles and their accompanying responsibilities can be formally laid down, for instance legally, in a contract or in professional or corporate codes of conduct. Since a person often has different roles in life he/she has various role responsibilities that can overlap.
* **Moral** responsibility: the responsibility that is based on moral obligations, moral norms or moral duties. It can extends beyond roles, and can also limit roles responsibility because with some roles immoral responsibilities may be associated.
* **Professional** responsibility: the responsibility that is based on one’s role as professional in as far it stays within the limit of what is morally allowed. It can be both active and passive.

**Passive responsibility**

Typical for passive responsibility is that the person who is held responsible must be able to provide an account why he/she followed a particular course of action, **why** he/she made certain decisions, he/she is held to **justify** his/her actions.

* **Accountability**: backward-looking responsibility in the sense of being **held to account for**, or justify one’s actions towards others.
* **Liability**: the state of being legally responsible for something.
* **Blameworthiness**: backward-looking responsibility in the sense of being a proper **target of blame** for one’s actions or the consequences of one’s actions. Usually four conditions need to apply:
  * **Wrong-doing**: the individual or the institution in question has violated a norm or did something wrong.
  * **Causal contribution**: the person who is held responsible must have made a causal contribution to the consequences for which he/she is held responsible. Not only an action, but also a failure to act may often be considered a causal contribution. A causal contribution is usually not a sufficient condition for the occurrence of the consequence under consideration.
  * **Foresee-ability**: a person who is held responsible for something must have been able to know the consequences of his/her actions. People cannot be held responsible if it is totally unreasonable to expect that they could possibly have been **aware** of the **consequences**.
  * **Freedom of action**: the one who is held responsible must have had freedom of action, he/she must not have acted under compulsion. Individuals are either not responsible, or less responsible, if they are **coerced** to take certain decisions.

**Active responsibility**

Responsibility before something has happened referring to a duty or task to care for certain state-of-affairs or persons. If someone is actively responsible for something he/she is expected to act in such a way that **undesired consequences** are **avoided** as much as possible and so that positive consequences are realized. Active responsibility is not only about preventing the negative effects of technology as about realizing certain positive effects. Mark Bovens' features of active responsibility:

* Adequate perception of threatened violations of norms.
* Consideration of the consequences.
* Autonomy, that is the ability to make one’s own independent moral decisions.
* Displaying conduct that is based on a verifiable and consistent code.
* Taking role obligations seriously.

**Ideals** are ideas which are particularly **motivating** and **inspiring** for the person having them, and which aim at achieving an **optimum** or maximum. Ideals cannot be entirely achieved but are strived for. Some of the ideals are directly linked to **professional** **practice** because they are closely allied to the profession or can only be aspired to by carrying out the profession. **Technological enthusiasm** is the ideal of wanting to develop **new** technological possibilities and take up technological **challenges**. Technological enthusiasm in itself is not **morally improper**, the inherent danger lies in the **possibile negative effects**. Engineers tend to strive for:

* **Effectiveness**: the extent to which an established goal is achieved.
* **Efficiency**: the ration between the goal achieved and the effort required.

The matter of whether effectiveness or efficiency is **morally worth pursuing** depends very much on the ends for which they are employed. So, although some engineers have maintained the opposite, the measurement of the effectiveness and efficiency of a technology is **value-laden**. **Human welfare** is the ideal of contributing or to augmenting human welfare (values such as health, environment, sustainability). As we have seen technological enthusiasm and effectiveness and efficiency are morally neutral; in both cases much depends on the **goals** for which technology is used and the **side-effects** so created. Human welfare confirms that the professional practice of engineers is not something that is morally neutral and that engineers do more than merely develop neutral means for the goals of others.

**Engineers vs managers**

Engineers are often salaried employees and they are usually hierarchically below managers. There are three models of dealing with this tension and the potential conflict between engineers and managers:

*   **Separatism**: the notion that scientists and engineers should apply the technical inputs, but appropriate management and political organs should make the value decisions. It is represented by the **tripartite model**:

    * The first segment contains the politicians and managers who establish the objectives for engineering projects and make available resources.
    * The second segment contains the engineers who take care of the designing, developing, creating of those projects or products.
    * The third segment contains the users.

    According to this model engineers can only be held responsible for the technical creation of products. The model is based on the assumption that the responsibility of engineers is limited to the **professional responsibility**.
* **Technocracy**: government by experts. The proposal is that engineers should take over the role of managers in the governance. The main issues are that:
  * It is not exactly clear what **unique expertise** engineers possess that permit them to legitimately lay claim to the role of **technocrats**.
  * Technocracy is **undemocratic** and **paternalistic**: that is the making of (moral) decisions for others on the assumption that one knows better what is good for them than those others themselves.
*   **Whistle-blowing**: the disclosure of certain abuses in a company by an employee in which he or she is employed, without the consent of his/ her superiors, and in order to remedy these **abuses** and/or to **warn** the public. The main issues are that:

    * It usually forces people to make big **sacrifices** and one may question whether it is legitimate to expect the average professional to make such sacrifices.
    * The **effectiveness** of whistle-blowing is often limited because as soon as the whistle is blown the **communication** between managers and professionals has inevitably been **disrupted**.

    The **guidelines** for whistle-blowing are according to De George:

    * The organization to which the would-be whistle-blower belongs will, through its product or policy, do serious and considerable **harm to the public**.
    * The would-be whistle-blower has identified that threat of harm, reported it to her immediate superior, making clear both the threat itself and the objection to it, and concluded that the **superior** will do **nothing effective**.
    * The would-be whistle-blower has exhausted/made use of as many **internal procedures** as the danger to others and her own safety make reasonable.
    * The would-be whistle-blower has evidence that would **convince** a reasonable, **impartial observer** that her view of the threat is correct.
    * The would-be whistle-blower has good reason to believe that revealing the threat will probably prevent the harm at **reasonable cost** (all things considered).

**The social context of technological development**

Engineers are just one of the many actors involved in technology development and cannot alone determine technological development and its social consequences, not all responsibility is theirs. There are other **actors**, persons or group that can make a decision how to act and that can act on that decision, that influence the direction taken by the technological development.

* Developers and producers of technology.
* **Users**, people who use a technology and who may formulate certain wishes or **requirements** for the functioning of a technology.
* **Regulators**, organizations (such as government) who formulate **regulations** that engineering products have to meet such as rulings concerning health and safety.

All these actors have certain **interests**: things actors strive for because they are **beneficial** or **advantageous** for them. **Stakeholders** are actors that have an interest (”a stake”) in the development of a technology, but who cannot necessarily influence the direction of technological development. The **technology assessment** (**TA**) is a systematic method for exploring future technology developments and assessing their potential societal consequences. At the same time the **Collingridge dilemma** refers to a **double-bind problem** to control the direction of technological development. On the one hand, it is often not possible to **predict** the consequences of new technologies in the early phases of technological development. On the other hand, once the (negative) consequences materialize it often has become very difficult to change the direction of technological development. One of the best method to overcome the Collingridge dilemma is the **constructive technology assessment** (**CTA**) in which TA-like efforts are carried out parallel to the process of technological development and are fed back to the development and design process. Among other things, this implies that stakeholders get a larger say in technological development.

***

# R3: Do artifacts have politics?

In controversies about technology and society, there is no idea more provocative than the notion that technical things have **political qualities**, that machines can be judged for the ways in which they can embody specific forms of power and authority. According to Lewis Mumford two types of technologies have recurrently existed side by side:

* **Authoritarian**: system-centered, immensely powerful, but inherently unstable.
* **Democratic**: a-centered, relatively weak, but resourceful and durable.

Technical systems are deeply interwoven in the conditions of modern politics. Certain technologies in themselves have political properties seem completely mistaken. What matters is not technology itself, but the **social** or **economic system** in which it is **embedded**: this maxim is the premise of the **social determination of technology** theory. It is true that **technologies** are **shaped** by **social** and **economic** **forces**, but also the **vice versa**: society changes and advances based on the technological advancements, humans adapt to technology. It follows two way in which artifacts can contain political properties.

**Technical arrangements as a forms of order**

Technological change expresses a panoply of human motives, not the least of which is the **desire** of some to have **dominion over others**, even though it may require an **occasional sacrifice** of cost-cutting and some violence to the norm of getting more from less. In the case like those of Moses’s low bridges and McCormick’s molding machines, technologies are seen as neutral tools that can be used well or poorly, for good, evil, or something in between. But what if a given device has been designed and built in such a way that it produces a set of consequences logically and temporarly prior to any of its professed uses? Some features in the design or arrangement of a device or system could provide a convenient means for **establishing patterns** of **power** and **authority** in a given setting. Withing a given category of the technological change there are two kinds of **choices** that can **affect** the relative **distribution** of **power**, **authority**, and **privilege** in a community:

* **Yes or no**: are we going to develop the new technology or not?
* **How**: decisions about specific features in the design or arrangement of a technical system after the decision to go ahead with it has already been made.

Making these choices is complex: not everyone is aware in the same way about how a technology will influence society. The highest level of choice is at the start, because once a standard is taken choices are more and more fixed. But at the start usually little is known about the consequences (**Collingridge Dilemma**). The things we call “**technologies**” are **ways** of **building order** in our world. In that sense technological innovations are similar to legislative acts or political foundings that establish a framework for public order that will endure over many generations.

**Inherently political technologies**

Some technologies are by their **very nature political** in a specific way: centralized or decentralized, egalitarian or in-egalitarian, repressive or liberating. As Lewis Mumford said. Usually, especially in the case of **large scale technology**, they are strongly linked to particular **institutionalized patterns** of power and authority. The idea we must now examine and evaluate is that certain kinds of technology do not allow such flexibility, and that to choose them is to choose a particular form of political life.

* According to **Engel** as a society adopted increasingly complicated technical systems as its material basis, the prospects for **authoritarian** ways of life would be greatly enhanced. Technology is inherently **hierarchical**, operating a machine necessitate order and coordination, granted by a hierarchical scheme where, at the top, there is the "**imperious authority**" of the machine (very extremist view). The adoption of a given technical systems actually requires the creation and maintenance of a particular set of **social conditions** as the operating environment of that system. Some kind of technology require their social environments to be structured in a particular way.
* According to **Marx** increasing **mechanization** will render **obsolete** the **hierarchical** division of **labor** and the relationship of **subordination** that, in his view, were necessary during the early stages of modern manufacturing.
* According to **Chandler** the construction on day-to-day operation of many systems require the development of a particular **social form**, a large scale **centralized**, **hierarchical** organization administered by **highly skilled managers**. If such systems are to work effectively, efficiently, quickly and safely certain requirement of internal social organization have to be fulfilled.

Further distinction to be made between conditions that are **internal** to the **workings** of a given technical system and those that are **external** to it. Engels’s thesis concerns internal social relations. Are the social conditions predicated said to be required by, or strongly compatible with, the workings of a given technical system? Are those conditions internal to that system or external to it (or both)? It is characteristic of societies based on large, complex technological systems, that **moral reasons** other than those of **practical necessity** appear increasingly **obsolete**, “idealistic” and irrelevant. In many instances, to say that some technologies are inherently political is to say that certain widely accepted reasons of practical necessity have tended to eclipse other sorts of moral and political reasoning. But can the internal politics of technology and the politics of the whole community be so easily separated?

The two varieties of interpretation outlined indicate how artifacts can have political qualities. In the first instance we noticed ways in which specific **features** in the **design** or arrangement of a device/system could provide a convenient means of **establishing patterns** of power and authority in a given setting. Technologies of this kind have a range of flexibility in the dimensions of their material form. In the second instance we examined ways in which the intractable **properties** of certain kinds of **technology** are strongly, perhaps unavoidably, **linked** to particular **institutionalized patterns** of power and authority. The initial choice about whether or not to adopt something is decisive in regard to its consequences. Both kinds of understanding are applicable in different circumstances. Within a particular technology some aspects may be flexible in their possibilities for society, while other aspects may be (for better or worse) completely intractable. The two varieties of interpretation examined can overlap and intersect at many points.

***

# R4: Why Privacy is Important?

According to Thomas **Scanlon** the first element of a theory of privacy is a characterization of the special interest we have in being able to be free from certain kinds of **intrusion**.

* Privacy is sometimes necessary to protect people’s **interest** in **competitive situations**.
* Someone may want to keep some aspect of his life or behavior private simply because it would be **embarrassing** for other people to know about it.

Examining various cases will not provide a complete understanding of the importance of privacy, for two reasons. These cases all involve relatively **unusual** sorts of **situations**, in which someone has something to **hide** or which information about a person might provide someone with a reason for **mistreating** him in some way. These cases do not capture the value of privacy in a normal or **ordinary** situation. We have a sense of privacy which is violated in such affairs that cannot be explained merely in terms of our fear of being embarrassed or disadvantaged in one of these obvious ways.

**Privacy and relationships**

The value of privacy is based on the idea that there is a close connection between our ability to control **who has access to** us and to **information** about us, and our ability to create and maintain different sorts of **social relationships** with different people. Privacy is necessary if we are able to maintain the variety of social relationships with other people that we want to have. These relationship have definite patterns of behavior. Our relationship with other people determine, in large part, how we act toward them and how they behave towards us. Different patterns of behavior are associated with different relationship. There is something **hypocritical** about such differences in behavior, underneath all the role-playing there is the “real” person and the various “**mask**” that we wear conceal our true self from some people. It is not merely accidental that we **vary our behavior** with different people according to the different social relationships that we have with them: the different patterns of behavior are (partly) what define the different relationships. The sort of relationship that people have one to another involves a conception of how it is **appropriate** from them **to behave** with each other. However one conceives one’s relations with other people is inseparable from the idea of how it is appropriate to behave with and around them, and what information about oneself it is appropriate for them to have. New types of social institutions and practices sometimes make possible new human relationships, which in turn make it appropriate to behave around people, and to say things in their presence, that would have been inappropriate before. One of the most important reason why we value privacy is our **ability** to **control** who has **access** to us, and who **knows** what about us, allow us to maintain the variety of relationship that we want to have. Even in most common circumstances, we regulate our behavior according to the kinds of relationships we have with the people around us. If we cannot control who has access to us, we cannot control the **patterns of behavior** we need to adopt or the kinds of relations with other people that we will have. What we cannot do is accept such a social role with respect to another person and then expect to retain the same degree of privacy relative to him that we had before.

Thomson suggests, as a **simplifying hypothesis**, that the right to privacy is itself a **cluster** of **rights**. It eliminates the right to privacy as anything distinctive. However the right to privacy has a different point than these other rights , but at the same time it can **overlap** with them. Even if it always overlap, we could still regard the right to privacy as a distinctive sort of right in virtue of the special kind of interest it protects. As a counter argument just think about a gossip about you that got leaked and so your privacy is violated: since the right that is violated in this case is not also a property right, or a right over the person, the simplifying hypothesis fails.

***

# R5: Privacy and Information Technology

People value their privacy and the protection of their **personal sphere** of life. They don’t want their **personal information** to be accessible. Recent advances in information technology threaten privacy and have reduced the amount control over personal data and open up the possibility of negative consequences. The combination of increasing power of new technology and the **declining clarity** and agreement on privacy give rise to problems concerning law, policy and ethics. The article focus on exploring the relationship between information technology and privacy, illustrating both specific threat that IT and innovation IT pose for privacy and indicate how IT itself might be able to overcome these privacy concerns by being developed in ways called “**privacy-sensitive**”. It will also discuss the role of **emerging technologies** in the debate, and account for the way in which moral debates are themselves affected by IT.

**Conceptions of privacy and the value of privacy**

A distinction could be made between:

* **Constitutional** (or decisional) privacy: **freedom** to make one’s own **decisions** without **interference** by others in regard to matters seen as intimate.
* **Tort** (or **informational**) privacy: interest of individuals in exercising **control over access** to **information** about themselves.

Statement about privacy can be either:

* **Descriptive**: used to **describe** the way people define situations and conditions of privacy.
* **Normative**: used to indicate that there must be **constraints** on the use of information of information processing.

Informational privacy in a normative sense refers typically to a non-absolute moral right of persons to have direct or indirect control over access to information about oneself, situations in which others could acquire information about oneself, and technology that can be used to generate, process or disseminate information about oneself.

The debates about privacy are almost always revolving around new technology. Two are the main opposite reactions: (1) we have **zero privacy** and there is no way we can protect it, so we should get used to and get over it, (2) our privacy is more **important** than ever and we must attempt to **protect** it. **Reductionist** accounts hold that the importance of privacy should be explained in terms of those other values. The opposite view holds that the **privacy** is **valuable in itself** and its importance is not derived from other considerations. More recently type of privacy has been proposed in relation to new information technology, which hold that there is a cluster of related moral claims underlying appeals to privacy, but there is no single essential core of privacy concerns. EU conceptualizes issues of informational privacy in terms of “data protection”, US in terms of “privacy”.

**Personal information** is data that is (or can be) **linked** to **individual** persons. It can be opposed with sensitive data (just think about password, that are not considered here as sensitive data even though it may contribute to privacy). This link between information and a person can be made in a **referential mode** or in a **non-referential** mode. The law is generally concerned with the referential use. If the legal definition of personal data is interpreted referentially, much of the data that could at some point brought to bear on persons would be unprotected.

Moral **reasons** for the **protecting** personal data and provide direct or indirect **control** over access to those data:

* **Prevention of harm**
* **Informational inequity**: individuals are usually not in a good position to **negotiate contracts** about the **use** of their **data** and do not have the means to check whether partners live up to the terms of the contract. Data protection laws, regulation and governance aim at establishing fair conditions for drafting contracts about personal data.
* **Informational injustice** **and discrimination**: personal information provided in one context may change its **meaning** when used in another context and may lead to discrimination and disadvantages.
* **Violation of personal choice and inherent human worth**: lack of privacy may expose individuals to **outside forces** that **influence** their **choices** and bring them to make decisions they would not have otherwise made (just think about mass surveillance). This affects their status as autonomous beings. Another problem is that the massive accumulation of data relevant to a person’s identity may give rise to the idea that we know a particular person. This constitutes an epistemic and moral immodesty, which fails to respect the fact that **human beings** are **subjects** with **private mental states** that have a certain quality that is inaccessible from an external perspective.

Data protection laws are in force in almost all countries. The basic moral principle underlying these laws is the requirement of informed consent. Since is impossible to guarantee compliance of all types of data processing in all these areas “**privacy-enhancing technologies**” (PETs) and identity management systems are expected to replace human oversight in many cases. New generations of privacy regulations now require standardly a “**privacy by design**” approach.

**Developments in information technology**

“**Information technology**” refers to automated systems for storing, processing and distributing information. The capacity of the technology has increased rapidly over the past decades (**digital/computer revolution**). The increased connectivity imposed by technology requires consideration (in a normative sense) of the desirability of this development, and evaluation of the potential for **regulation** by technological institutions. Information technology consists of a complex system of socio-technical practices.

The internet was originally designed for exchanging information and not for the purpose of separating information flows. A major theme in the discussion of internet privacy is the use of cookies, small pieces of data that web sites store on user’s computer, in order to enable personalization of the site. However, some cookies can be used to track the user across multiple web sites (**tracking cookies**), enabling for example advertisements. It is not always clear what the generated information is used for. Laws requiring user consent for the use of cookies are not always successful. In **cloud computing**, both data and programs are online, and as data are located elsewhere in the world it is not even always obvious which law is applicable, and which authorities can demand access to the data.

**Social network** sites invite the user to generate more data, to increase the value of the site. User are tempted to exchange their personal data for the benefits of using services, and provide both this data and their attention as payment for the services. One way of limiting the temptation of users to share is requiring default privacy settings to be strict. Such restrictions limit the value and usability of the social network sites themselves, and may reduce positive effects of such services.

Users generate loads of data when online, not only explicitly entered by them but also statistics on their behavior. **Data mining** can be employed to extract **patterns** from such data that may affect the online experience, but it also may impact the user in completely different contexts. Big data may be used in **profiling** the user, creating patterns of typical combinations of users properties, which can then be used to predict interests and behavior. These derivations could then in turn lead to inequal treatment or discrimination. Specific challenges, therefore, are:

* How to obtain **permission** when the user does not explicitly engage in a transaction (as in case of surveillance).
* How to prevent “**function creep**”, that is data being used for **different purposes** after they are collected (as may happen for example with DNA databases).

**Mobile devices** typically contain a range of data-generating **sensors**, including GPS, movement sensors and camera. It is supposed that the user knows when these sensors are active or not, but malicious software might be avoiding this. In general “reconfigurable technology” that handles personal data raises the question of user knowledge of the configuration.

Many devices contain **chips** and/or are connected in the **Internet Of Things**. **RFID** (radio frequency identification) chips can be read from a limited distance. EU and US passports have RFID chips with protected biometric data. Even dumb chips (chips that only contain a number) could be used to **trace a person** once it is known that he carries an item containing a chip. This is just the tip of the iceberg: many devices contain sensors which may communicate with the company in order to gather data.

Government and public administration have undergone radical transformations as a result of the availability of advanced IT systems as well (biometric passports, online **e-government** services, voting systems ..). In polling stations, the authorities see to it that the voter keeps the vote private, but such surveillance is not possible when **voting** by mail or online, and it cannot even be enforced by technological means, as someone can always watch while the voter votes. In this case, privacy is not only a right but also a duty, and information technology developments play an important role. In a broader sense, **e-democracy** initiatives may change the way privacy is viewed in the political process.

Information technology is used for all kinds of **surveillance** tasks. It can be used to augment and extend traditional surveillance systems, or to create a whole new kind of surveillance: "surveillance capitalism" where social media and other online systems are used to gather large amounts of data about individuals. In addition to the private surveillance, also governments uses surveillance techniques at a large scale, either by intelligence services or law enforcement.

**How can information technology itself solve privacy concerns?**

Whereas **information technology** is typically seen as the cause of privacy problems, there are also several ways in which it can **help** to solve these problem. There are guidelines or best practices that can be used for **designing privacy-preserving systems**. Such possibilities range from ethically informed design methodologies to using encryption to protect personal information from unauthorized use.

More recently, approaches such as “**privacy engineering**” extend the privacy by design approach by aiming to provide a more practical, deployable set of methods by which to achieve system-wide privacy. The privacy by design provides high-level guidelines in the form of principles that have at their core “data protection needs to be viewed in proactive rather than reactive terms, making privacy by design preventive and not simply remedial”. This does not means that if these methodologies are followed the resulting IT system will be privacy friendly, still some issues are open:

* Different people will **interpret** the principles differently.
* During the **implementation** phase software bugs are introduced, that can
* It is very hard to **verify** whether an **implementation** **meets** its design/**specification**.

Some other specific solutions aim at increasing the level of awareness and consent of the user.

A growing number of **software tools** are available that provide some form of privacy (usually **anonymity**), knowing as **privacy enhancing technologies**. In Tor, messages are encrypted and routed along numerous different computers. Similarly, in Freenet content is stored in encrypted form from all users of the system (but keys are not shared). They employ cryptographic techniques and security protocols in order to ensure their goal. Privacy enhancing technologies also have their downsides as they are susceptible to an attack. Moreover, configuring such software tools correctly is difficult for average user. Another option for providing anonymity is the **anonymization of data** through special software. But it is very hard to anonymize data in such a way that all links with an individual are removed and the resulting anonymized data is still useful for research purposes.

Modern **cryptography** **techniques** are essential in any IT system that needs to store personal data. On of the main problem is that cryptographic schemes become outdated by faster computers. Below are listed two new techniques:

* **Homomorphic encryption** allows a data processor to process encrypted data, so that users could send personal data in encrypted form and get back some useful results in encrypted form.
* **Blockchain technology**, although focused on data integrity and not inherently anonymous, enables many privacy-related applications. It is a distributed ledger that stores transactions in a non-reputable way, without the use of a trusted third party. Cryptography is used to ensure that all **transactions** are “**approved**” by members of the blockchain and stored in such way that they are linked to previous transactions and cannot be removed.

In the era of big data correct information about users has an increasing monetary value. ”**Single sign on**” make it easy for users to connect to online services using a single **online identity** that is usually directly linked to the **real identity** of the user. Requiring a direct link between online and ‘real world’ identities is problematic from a privacy perspective, because they allow profiling of users. A better solution would be the use of attribute-based authentication. Blockchain technology is used to make it possible for users to control a digital identity without the use of a traditional trusted third party.

**Emerging technologies and our understanding of privacy**

There are future and emerging technologies that may have en even more profound impact (for example brain-computer interfaces). Given the future, and even current, level of informational connectivity, it is feasible to protect privacy by trying to hide information from parties who may use it in undesirable way. It may be more feasible to protect privacy by **transparency** by **requiring actor to justify decisions** made about individuals thus insisting that decisions are not based on **illegitimate information**. It may well happen that citizens, in turn, start data collection on those who collect data about them. The open source movement may also contribute to transparency of data processing. Transparency can be seen as a **pro-ethical condition** contributing to privacy. According to the precautionary principle the burden of proof for absence of irreversible effects of information technology on society would lie with those advocating the new technology. Precaution could then be used to impose restrictions at a regulatory level.

***

# R6: Big Data’s End Run around Anonymity and Consent

Big data promises to deliver **analytic insights** that will add to the stock of scientific and social scientific knowledge, significantly improve decision making and greatly enhance individual self-knowledge and understanding. They have been perceived as a threat to fundamental values such as autonomy, fairness, justice, property, solidarity and privacy. Taking as a given that big data implicates important **ethical** and **political** values, we direct our focus instead on attempts to avoid or mitigate the conflicts that may arise. **Anonymization** seems to take data outside the scope of privacy, as it no longer maps onto **identifiable subjects**, while allowing information subjects to give or withhold **consent**, giving them (in theory) **control** over the information they want to share. This fits perfectly the common conception of privacy as: "control over information about oneself". In practice, however, anonymity and consent have proven **elusive**, as time and again critics have revealed fundamental problems in implementing both.

* In the case of anonymity even where strong guarantees of anonymity can be achieved, common applications of big data proves that even when individuals are not ‘identifiable’, they may still be ‘**reachable**’, may still be comprehensibly **represented** in records that detail their **attributes** and **activities**, and may be subject to consequential inferences and predictions taken on that basis.
* In the case of **consent,** it is absurd think that notice and consent can **fully specify** the **terms** of consent between data collector and data subject. This consideration highlights the inefficacy of consent as a matter of individual choice.

The conclusion is that procedural approaches cannot replace policies based on moral and political principles that serbe specific contextual goals and values.

**Definitions and background theory** The term “**big data**” better reflects a **paradigm** than a particular technology, method, or practice. It is a way of thinking about knowledge through data and framework for supporting decision making, rationalizing action, and guiding practice. It is a **belief** in the **power** of finely **observed patterns**, structures, and models drawn inductively from massive datasets.

There is some disagreement over how important privacy is among the various ethical and political issues raised by big data. The real problems include how we use the data, whether it is fair to treat people as part of a group. **Privacy** is the **requirement** that information about people (‘**personal information**’) **flows appropriately**, where appropriateness means in accordance with **informational norms**. Informational norms prescribe information flows according to **key actors**, types of information and **constraints** under which flow occurs (‘**transmission principles**’). Control over information about oneself is not presumed unless the other parameters (actors and information types) warrant it. Contextual informational norms generally are not fixed and static. Big data involves practices that have radically disrupted informational flows. **Privacy**, understood as **contextual integrity**, is fundamentally for immediately alerts us to the ways any practice conflicts with the expectations we may have based on informational flows norms. The point is that to distinguish between technologies that are not disruptive, a norm-based account of privacy, such as contextual integrity, must offer a basis for drawing such distinctions.

**Anonymity** obliterates the link between data and a specific person not so much to protect privacy but, in a sense, to **bypass** it entirely. The promise of anonymity is impossible to fulfill if individual records happen to contain information that **uniquely distinguishes** a person enough or extremely rich data that necessarily map onto specific individuals. If anonymity can be effectively guaranteed is outside of the scope of the extract, we assume that the problem of anonymization, classically speaking, has been solved (even if it has obviously not). Basically, assuming that anonymity works, is it worth it to protect us using it, or is it useless? The value of anonymity inheres to something we called ‘**reachability**’.

An **anonymous identifier** (**AnID**) is a unique persistent identifiers that differ from those in common and everyday use. They (in theory) **avoid association** with **records** of **other institutions**, but you are still identified within the institution that gave you that AnID. The history of the **social security number** is instructive: as a unique number is assigned to all citizens, the number served as a convenient identifier that other institutions could adopt for their own administrative purpose. So it is now perceived as sensitive. Any random string that acts as a unique persistent identifier should be understood as a pseudonym rather than an AnID.

A further worry is that the **comprehensiveness** of the records maintained by especially large institutions, that contain no identifying information, may become so **rich** that they subvert the very meaning of anonymity. Companies have no particular interest in who someone is because their ability to **tailor** their **offering** and services to individuals is in no way limited by the absence of such information. We can no longer turn to anonymity (or **pseudonymity**) to pull datasets outside the scope of privacy regulations and debate.

Even without names, personal information or anonymous identifiers, thanks to **data mining** a lot can be **inferred** from a given big enough data set. Insights drawn from big data can furnish additional facts about an individual without any knowledge of their specific identity. This renders the traditional protections afforded by anonymity (again, more accurately, pseudonymity) much less effective.

**Research** on some attributes based on anonymous data may provide institutions with **new paths** by which to **infer** precisely those attributes that were previously impossible to associate with specific individuals. Basically a research meant to guarantee anonymity enables institutions to infer characteristics that were previously "protected".

**Informed consent** is believed to be an effective means of **respecting individuals** as **autonomous decision makers**. But the act of consenting itself does not protect and support autonomy. It is understood as a crucial mechanism for ensuring privacy, it is a natural corollary of the idea that privacy means control over information about oneself. The **Fair Information Practice Principles** (**FIPPs**) are first principles of informed consent in privacy law. These principles demand that data subjects be informed about **who** is collecting, **what** is being collected, **how** information is being used and shared, and whether information collection is voluntary or required. The Internet challenged them a lot, eventually coming up with **privacy policies**, but the debate on how to make them better is still active. We accept that informed consent is a useful privacy measure in certain circumstances and against certain threats and that existing mechanisms can and should be improved, but, against the challenges of big data, consent, by itself, has little traction (much like the approach taken on anonymity).

There is little value in a protocol for informed consent that does not meaningfully model choice and, in turn, autonomy (**true freedom** of **choice**). For individuals to make considered decisions about privacy in this environment, they need to be informed about the types of information being collected, with whom it is shred, under what constraints, and for what purposes. At the same time plain-languages notices cannot provide information that people need to make such decisions. This is the **transparency paradox**: even the few people want to go in detail and actually read the policies, will most likely not understand them.

The machination of big data make difficult to describe **information practices** in ways that are **relevant** to privacy so that individuals grant or withhold consent, because data moves in unpredictable ways and its value is not always recognized at collection time. The company may extract further information by collaborating with third parties. Characterizing the type of information is even tougher. The value of big data lies in the **unexpectedness** of the insights that it can reveal. Basically, the flows of the informational norms are not well defined unless recipients and transmission principles are specified (which is almost never the case). Consent under those conditions is not meaningful.

The volunteered information of the few can unlock the same information about the many. This is the **tyranny** of **minority**. Even if we accept that individuals can make informed, rational decisions concerning their own privacy, there decisions affect what institutions can know about others. Multiple attributes can be inferred globally when as few as 20% of the users reveal their attribute information. Once a **critical threshold** has been reached, data collectors can rely on more easily observable information to situate all individuals according to these patterns, rendering irrelevant whether or not those individuals have consented to allowing access to the critical information in question. Withholding consent will make no difference to how they are treated.

* Some people think that **privacy** and **big data** are **incompatible**, to them, the points shown before just enforce the need to remove privacy from its pedestal and allow the glorious potential of big data to be fulfilled. This point is wrong since these people adhere to a mistaken conception of privacy, often seen as control or as secrecy.
* Others say that we should remain concerned about ethical issues raised by big data, that, while privacy may be a lost cause, the real problems arise with use. Basically **allow data collection** and **prevent misuse**.

Our point of view: we are not yet ready to give up on privacy, nor completely on anonymity and consent. **Background** and **context-driven rights** and **disadvantage** have been neglected in favor of anonymity and consent to the **disadvantage** of individuals and social integrity. They should instead be the foundation of how privacy is guaranteed:

* There should be a set of background and context driven rights and obligations that must be respected.
* The user should only consent to data collection or sharing that violates this set of obligations. This sharing should be very limited in time and purpose.
* It is up to the data user to justify why certain data shall be collected.
* Where, for example, anonymizing data, adopting pseudonyms, or granting or withholding consent makes no difference to outcomes for an individual, we had better be sure that the outcomes in question can be defended as **morally** and **politically** **legitimate**.
* When anonymity and consent do make a difference, we learn from the domain of scientific integrity that simply because someone is anonymous or pseudonymous or has consented does not by itself legitimate the action in question.

***

# R7: Why software should be free?

Software developers often consider decision about the use of a software on the assumption that the criterion is to **maximize developer’s profit**. The government has adopted this criterion stating that the **program** has an **owner**, typically a corporation associated with its development. It is necessary to perform a cost-benefit analysis on behalf of society as a whole, taking into account individual freedom ad well as production of material goods, in order to see who is helped and who is hurt by recognizing owners of software, why and how much.

**How owner justify their power**

Those who benefit from the current system offer two arguments in support of their claims: the **emotional** and the **economic** one.

* The **emotional** argument does not require serious refutation since the **feeling** of attachment is cultivated by programmers when it suits them, since they are willing to **give up all rights** to a company in exchange for a salary.
* The economic argument is based on a **threat**: “if you don’t allow me to **get rich** by programming I will not program anymore”. This formulation starts by comparing the social utility of a proprietary program with that of no program, and then concludes that proprietary software development is beneficial. The fallacy is in comparing two outcomes (proprietary software vs no software) assuming that there are no other possibilities.

**The argument against having owners**

“Should development of software be linked with having owners to restrict the use of it?”. In order to answer it we have to judge the effect on society of each activity independently (effect of developing the software, effect of restricting its use). The point is that if restricting the distribution of a program already developed is harmful to society overall, then an ethical software developer will reject the option of doing so. In order to determine the effect of restricting sharing we have to compare two different world. A simple counterargument could be that “the **benefit** to the **neighbor** of giving him/her a copy of the program is **cancelled** by the **harm** done to the **owner**”, assuming that the harm and the benefit are equal in magnitude. Just think about free roads and toll roads: free roads are cheaper to construct, to run, safer, and more efficient to use and also in a poor country, tools may make the roads unavailable to many citizens. The road without tools are preferable for society. The same argument can be applied to software development: restricting the distribution of a program makes it more expensive to construct, to distribute and less efficient to use. It will follow that the program construction should be encouraged in some other way.

**The harm done by obstructing software**

Consider that a program has been developed and now society must choose either to make it proprietary or to allow free sharing and use. Assume that the existence of the program and its availability is a desirable thing. **Restriction** on the **distribution** and **modification** of the program may lead to **negative effects**. Three level of material harm can be defined:

* Fewer people use the program.
* None of the users can adapt or fix the program.
* Other developers cannot learn from the program, or base new work on it.

Each level of **material harm** has a concomitant form of **psychosocial harm**, thus there is no limit to the harm that proprietary software can do. Furthermore, the material harm waste part of the value that the program could contribute.

Level 1: **Obstructing use of programs**

A copy of a program has nearly zero marginal cost, so in a free market, it would have nearly zero price. The total contribution of a program to society is reduced by assigning an owner to it, since each potential user of the program may decide not to pay for it and not to use it. This harms the person without benefiting anyone. When a user chooses to pay, this is a zero-sum transfer of wealth between two parties. A centrally imposed fee for software distribution becomes a powerful disincentive.

Level 1: **Damaging social cohesion**

Suppose that both you and your neighbor would find it useful to run a certain program, in ethical concern you should feel that proper handling of the situation will enable both to use it. A proposal to permit only one of you to use it is **divisive**. People that makes such choices feel internal psychological pressure to justify them, by **downgrading** the **importance** of **helping one another**, thus the public spirit suffers. Many users recognize the wrong of refusing to share, so they deice to ignore the licenses and laws and **share programs anyway**, they know that they must **break the** **law** in order to be good neighbors, but at the same time finds the law autoritative, and conclude that they are shameful. Programmers also suffer psychosocial harm knowing that many users will not be allowed to use their work leading to an attitude of cynicism of denial.

Level 2: **Obstructing custom adaptation of programs**

Source code is useful (at least potentially) to every user of a program. But most commercially available software isn't available for modification, even after you buy it. This causes lots of wasted time because people need to rewrite software from scratch instead of editing existing software. Also, not everyone can afford it and giving up causes psychosocial harm.

Level 3: **Obstructing software development**

Software development used to be an **evolutionary process**. Now the existence of owners prevents new practitioners from studying existing programs to learn useful techniques or even how large programs can be structured. Owners **obstruct education**. In any intellectual field, one can reach greater heights by standing on the shoulders of others. But that is no longer generally allowed in the software field. The associated psychosocial harm affects the spirit of scientific cooperation.

**It does not matter how sharing is restricted, if it succeeds in preventing use, it does harm.**

**Software should be free**

A business making proprietary software stands a chance of success in its own narrow terms, but it is not what is good for society.

**Why people will develop software**

If we **eliminate copyright** as a means of encouraging people to develop software, at first **less** software will be developed, but that software will be **more useful**. There are fields of study and art in which there is little chance to become rich, which people enter for their fascination or their perceived value to society.

**Funding free software**

Institutions that pay programmers do not have to be software houses. Many other **institutions** already exist that can do this: universities or new institutions such as the **Free Software Foundation**. **Universities** conducts many programming projects that could be funded by the same **government** contracts and grants that now support proprietary software development. It is common today for university researchers to get grants to develop a system and then start companies where they really finish the project and make it usable. New institutions such as the Free Software Foundation can also fund programmers. Most of the Foundation's funds come from users buying tapes with free and replicabile software. The Free Software Foundation is a charity, and its income is spent on hiring as many programmers as possible. Because the Foundation is a charity, programmers often work for the Foundation for half of what they could make elsewhere. They do this because we are free of bureaucracy, and because they feel satisfaction in knowing that their work will not be obstructed from use. This confirms that programming is among the most fascinating of all fields, along with music and art. We don’t have to fear that no on will want to program.

**What do users owe to developers?**

There are many good reasons for users of software to feel a moral obligation to contribute to its support. Developers of free software are contributing to the user’ activities, and it is both fair and in the long-term interest of the users to give them funds to continue. However, this does not apply to proprietary software developers, since obstructionism deserves a punishment rather than a reward. A developer can either deserve a reward or demand it, but not both: If you write proprietary and demand reward, you don't deserve. The point is that the developer of useful software is entitled to the support of the users, but any attempt to turn this moral obligation into a requirement destroys the basis for the obligation.

**What is software productivity?** If software were free, there would still be programmers, but perhaps fewer of them. However, this will not be a problem. Free software would require far fewer programmers to satisfy the demand, because of increased software productivity at all levels:

* Wider use of each program that is developed.
* The ability to adapt existing programs for customization instead of stating from scratch.
* Better education of programmers.
  * The elimination of duplicate development effort.

“Software productivity” can mean two different things: the overall productivity of all software development (that is what society would like to improve by eliminating the artificial obstacles to cooperation) or the productivity of individual projects.

**Is competition inevitable**

It is inevitable that people will try to complete? Perhaps it is, but competition is not harmful, combat is. Competition can consist of trying to **achieve ever more**, to outdo what other have done. This can **benefit everyone**, as long as the spirit of good **sportsmanship** is maintained (constructive competition: “Let the best person win”). Competition becomes combat when the competitors begin trying to impede each other instead of advancing themselves. Proprietary software is harmful because it is a form of combat among the citizens of our society. Hold back information that could help everyone advance is a form of combat.

**The question of premises**

The paper makes the assumption that a **user** of a software is **no less important** than an **author**, or even an author’s employer (their interests and needs have the equal weight). This premise is not universally accepted. For those who believe that the owners are more important than everyone else, this paper is simply irrelevant.
